{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0a1031da6ceaf3",
   "metadata": {},
   "source": [
    "# Join SFD map with a point source catalog\n",
    "\n",
    "We need LSDB for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:35:10.848429Z",
     "start_time": "2023-12-01T19:35:09.694425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/malanche/.virtualenvs/sfd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-18 16:39:12,708\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import dask\n",
    "import lsdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from hipscat.pixel_math.hipscat_id import HIPSCAT_ID_COLUMN, hipscat_id_to_healpix\n",
    "from lsdb.core.crossmatch.abstract_crossmatch_algorithm import AbstractCrossmatchAlgorithm\n",
    "from ray.util.dask import enable_dask_on_ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06df66-6d9e-4749-9261-3d94c4892515",
   "metadata": {},
   "source": [
    "Linear search is faster than `np.searsorted` for most of the cases.\n",
    "See some benchmarks and tests here:\n",
    "https://github.com/hombit/linear-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627a0e5e-07c8-4bea-8618-af27bcf1425b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2], dtype=uint64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, uint64\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "# @njit(uint64[:](uint64[:], uint64[:]), boundscheck=False, fastmath=True)\n",
    "@njit(boundscheck=False, fastmath=True)\n",
    "def linear_search_numba(a: NDArray, b: NDArray) -> NDArray:\n",
    "    \"\"\"Find the place index of each element of b in a. Both a and b are sorted.\"\"\"\n",
    "\n",
    "    # Initialize the index with the last index of the target array\n",
    "    idx = np.full(shape=b.size, fill_value=a.size, dtype=np.uint64)\n",
    "\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return idx\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    while i < a.size and j < b.size:\n",
    "        while j < b.size and b[j] < a[i]:\n",
    "            idx[j] = i\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "# Run first time to compile\n",
    "linear_search_numba(np.zeros(2, dtype=np.uint64), np.zeros(2, dtype=np.uint64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf405b4b956ede9",
   "metadata": {},
   "source": [
    "Data paths\n",
    "\n",
    "Hardcoded path to PS1 DR2 object table (OTMO) and SFD map at PSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372a234f50ca540e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:35:10.854338Z",
     "start_time": "2023-12-01T19:35:10.849426Z"
    }
   },
   "outputs": [],
   "source": [
    "STARS_PATH = Path('/ocean/projects/phy210048p/shared/hipscat/catalogs/ps1/ps1_otmo')\n",
    "\n",
    "# Use SDSS DR16 Quasar catalog for a while...\n",
    "# STARS_PATH = Path('/ocean/projects/phy210048p/shared/hipscat/catalogs/agns_dr16q_prop_May16')\n",
    "\n",
    "# Fixed order 14 SFD map\n",
    "# SFD_PATH = Path('/ocean/projects/phy210048p/shared/hipscat/catalogs/sfd/sfd_order14_map')\n",
    "# Multiorder SFD map, interpolation error is <1%\n",
    "SFD_PATH = Path('/ocean/projects/phy210048p/shared/hipscat/catalogs/sfd/sfd_multiorder_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ab798f16ebbe1",
   "metadata": {},
   "source": [
    "### We are using LSDB's cross-matching interface for joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcfc4851c1ebc0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:35:14.625499Z",
     "start_time": "2023-12-01T19:35:14.619368Z"
    }
   },
   "outputs": [],
   "source": [
    "class JoinWithContinuousMap(AbstractCrossmatchAlgorithm):\n",
    "    DISTANCE_COLUMN_NAME = '_DIST'\n",
    "    \n",
    "    def crossmatch(\n",
    "            self,\n",
    "            search_algo: Literal['auto', 'numpy', 'linear'] = 'auto',\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Perfrom cross-match\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        search_algo : 'auto' or 'numpy' or 'linear'\n",
    "            Index join algorithm, one of the followoing:\n",
    "            - 'numpy' - `np.searsorted(right, left, side='right')`,\n",
    "              it is faster for smaller left tables it is\n",
    "              O(n_left * log(n_right)). Right table hipscat index must\n",
    "              be sorted.\n",
    "            - 'linear' - linear search algorithm, it is faster for\n",
    "              smaller right tables, it is O(n_left + n_right).\n",
    "              Both tables' hipscat index must be sorted.\n",
    "            - 'auto' - use algoithm which is faster by the following\n",
    "              heuristics based on algorithmic complexities with some\n",
    "              coefficient driven by experiments on sizes between\n",
    "              thousand and million:\n",
    "              if `n_left + n_right > 5 * n_left * lb(n_right)` use\n",
    "              'numpy', and 'linear' otherwise.\n",
    "        \"\"\"        \n",
    "        # Check that both catalogs are sorted by HIPSCAT_ID_COLUMN\n",
    "        assert np.all(np.diff(self.left.index) > 0)\n",
    "        assert np.all(np.diff(self.right[HIPSCAT_ID_COLUMN]) > 0)\n",
    "   \n",
    "        if search_algo == 'auto':\n",
    "            if self.left.shape[0] + self.right.shape[0] > 5.0 * self.left.shape[0] * np.log2(self.right.shape[0]):\n",
    "                search_algo = 'numpy'\n",
    "            else:\n",
    "                search_algo = 'linear'\n",
    "        if search_algo == 'numpy':\n",
    "            idx = np.searchsorted(\n",
    "                self.right[HIPSCAT_ID_COLUMN],\n",
    "                self.left.index,\n",
    "                side='right',\n",
    "            ) - 1\n",
    "        elif search_algo == 'linear':\n",
    "            idx = linear_search_numba(\n",
    "                np.asarray(self.right[HIPSCAT_ID_COLUMN], dtype=np.uint64),\n",
    "                np.asarray(self.left.index, dtype=np.uint64),\n",
    "            ) - 1\n",
    "        else:\n",
    "            raise ValueError(f'Unknown search algo \"{search_algo}\"')\n",
    "            \n",
    "        \n",
    "        # np.searchsorted output must be between 0 and N,\n",
    "        # so we are checking -1 case only\n",
    "        assert np.all(idx >= 0)\n",
    "        \n",
    "        self._rename_columns_with_suffix(self.left, self.suffixes[0])\n",
    "        self._rename_columns_with_suffix(self.right, self.suffixes[1])\n",
    "        \n",
    "        left_join_part = self.left.reset_index()\n",
    "        right_join_part = self.right.iloc[idx].reset_index(drop=True)\n",
    "        \n",
    "        out = pd.concat(\n",
    "            [\n",
    "                left_join_part,\n",
    "                right_join_part,\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        out[self.DISTANCE_COLUMN_NAME] = 0.0\n",
    "        out.set_index(HIPSCAT_ID_COLUMN, inplace=True)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f36a742-3401-4ba9-b47e-a36389c9e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a command for dashboard ssh-tunneling\n",
    "\n",
    "import socket\n",
    "from getpass import getuser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "local_addr = '127.0.0.1:8787'\n",
    "remote_host = 'bridges2.psc.edu'\n",
    "\n",
    "\n",
    "def print_client_info(client):\n",
    "    display(client)\n",
    "    \n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as s:\n",
    "        s.connect(('1.1.1.1', 53))\n",
    "        ip = s.getsockname()[0]\n",
    "    username = getuser()\n",
    "    dashboard_port = urlparse(client.dashboard_link).port\n",
    "\n",
    "    print(f'''\n",
    "    Copy-paste and run in your terminal:\n",
    "\n",
    "    ssh -N -L {local_addr}:{ip}:{dashboard_port} {username}@{remote_host}\n",
    "\n",
    "    And open this URL in your browser to see the dashboard:\n",
    "    http://{local_addr}/\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8215a3d8a4422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:35:17.023221Z",
     "start_time": "2023-12-01T19:35:15.219221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 16:39:15,374\tINFO worker.py:1664 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1:8265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/malanche/.virtualenvs/sfd/lib/python3.9/site-packages/dask/config.py:742: FutureWarning: Dask configuration key 'shuffle' has been deprecated; please use 'dataframe.shuffle.algorithm' instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# I have some connect issues runiing on PSC...\n",
    "dask.config.set({\n",
    "    'distributed.comm.timeouts.connect': '60s',\n",
    "    'distributed.comm.timeouts.tcp': '60s',\n",
    "})\n",
    "\n",
    "context = ray.init()\n",
    "print(context.dashboard_url)\n",
    "with enable_dask_on_ray():\n",
    "\n",
    "# with SLURMCluster(\n",
    "#     # Number of Dask workers per node\n",
    "#     processes=4,\n",
    "#     # Regular memory node type on PSC bridges2\n",
    "#     queue=\"RM\",\n",
    "#     # dask_jobqueue requires cores and memory to be specified\n",
    "#     # We set them to match RM specs\n",
    "#     cores=128,\n",
    "#     memory=\"256GB\",\n",
    "#     walltime=\"12:00:00\",\n",
    "# ) as cluster:\n",
    "#     # Run multiple jobs\n",
    "#     # cluster.scale(jobs=10)\n",
    "#     # Allow to run more jobs\n",
    "#     cluster.adapt(maximum_jobs=10)\n",
    "\n",
    "#     with Client(cluster) as client:\n",
    "# with Client(n_workers=4) as client:\n",
    "        # print_client_info(client)\n",
    "\n",
    "        stars = lsdb.read_hipscat(STARS_PATH)\n",
    "        sfd = lsdb.read_hipscat(SFD_PATH)\n",
    "        matched = stars.crossmatch(\n",
    "            sfd,\n",
    "            algorithm=JoinWithContinuousMap,\n",
    "            search_algo='auto',\n",
    "        )\n",
    "        mean_sfd = matched._ddf[f'ebv_{sfd.name}'].mean().compute()\n",
    "mean_sfd\n",
    "\n",
    "with open('ps1-multiorder.txt', 'w') as f:\n",
    "    f.write(f'{mean_sfd = }\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe19b97-c23d-432c-b39f-477668169e53",
   "metadata": {},
   "source": [
    "Alternative approach: use dustmaps package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9f017-042f-4459-9ac9-b68a1ae2af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord\n",
    "from dustmaps.sfd import SFDQuery\n",
    "\n",
    "# Get original SFD FITS file location, INPUT_DIR\n",
    "from paths import *\n",
    "\n",
    "\n",
    "def worker(df, query):\n",
    "    coord = SkyCoord(\n",
    "        ra=df[stars.hc_structure.catalog_info.ra_column],\n",
    "        dec=df[stars.hc_structure.catalog_info.dec_column],\n",
    "        unit='deg',\n",
    "    )\n",
    "    ebv = query(coord)\n",
    "    return pd.DataFrame(dict(ebv=ebv), index=df.index)\n",
    "\n",
    "\n",
    "# context = ray.init()\n",
    "# print(context.dashboard_url)\n",
    "with enable_dask_on_ray():\n",
    "\n",
    "# with Client(n_workers=24) as client:\n",
    "    # print_client_info(client)\n",
    "    \n",
    "    query = dask.delayed(SFDQuery, pure=True, traverse=False)(INPUT_DIR)\n",
    "    \n",
    "    stars = lsdb.read_hipscat(STARS_PATH)\n",
    "    values = stars._ddf.map_partitions(worker, query, meta={'ebv': np.float32})\n",
    "    mean_values = values.mean().compute()\n",
    "    \n",
    "print(mean_values)\n",
    "\n",
    "with open('ps1-dustmaps.txt', 'w') as f:\n",
    "    f.write(f'{mean_values = }\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0e0accdf0433e",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "First, we check that both hipscat indexes and SFD pixel index-order pair are all consistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148e51dbe701336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:35:17.940187Z",
     "start_time": "2023-12-01T19:35:17.930010Z"
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(\n",
    "    hipscat_id_to_healpix(result[f'_hipscat_index_{SFD_NAME}'], result[f'pixel_Norder_{SFD_NAME}']),\n",
    "    result[f'pixel_Npix_{SFD_NAME}'],\n",
    ")\n",
    "np.testing.assert_array_equal(\n",
    "    hipscat_id_to_healpix(result.index, result[f'pixel_Norder_{SFD_NAME}']),\n",
    "    result[f'pixel_Npix_{SFD_NAME}'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234fa447104f552d",
   "metadata": {},
   "source": [
    "Check that SFD map values are close enough to the ones from `dustmap` module.\n",
    "The difference must be below 16% for fixed order and 1% for multiorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a6295bfe4d454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:35:54.850461Z",
     "start_time": "2023-12-01T19:35:54.809220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validate\n",
    "from astropy.coordinates import SkyCoord\n",
    "from dustmaps.sfd import SFDQuery\n",
    "\n",
    "sfd_query = SFDQuery(INPUT_DIR)\n",
    "coord = SkyCoord(ra=result['ra_small_sky_order1'], dec=result['dec_small_sky_order1'], unit='deg')\n",
    "dustmaps_sfd_values = sfd_query(coord)\n",
    "\n",
    "diff = (\n",
    "    np.abs(result[f'ebv_{SFD_NAME}'] - dustmaps_sfd_values)\n",
    "    / np.where(result[f'ebv_{SFD_NAME}'] > dustmaps_sfd_values, result[f'ebv_{SFD_NAME}'], dustmaps_sfd_values)\n",
    ")\n",
    "i = np.argsort(diff)[::-1]\n",
    "display(result.assign(diff=diff, ebv_dustmap=dustmaps_sfd_values).iloc[i[:10]])\n",
    "diff.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a22c7a7a4502b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:55:24.553784Z",
     "start_time": "2023-12-01T16:55:10.332158Z"
    }
   },
   "outputs": [],
   "source": [
    "area17 = 4 ** (17 - sfd._ddf['pixel_Norder'].astype(np.uint64))\n",
    "area17.sum().compute(), 12 * 4 ** 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebaa48716c2c02a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T16:57:12.891745Z",
     "start_time": "2023-12-01T16:55:24.594411Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "for norder in range(8, 18):\n",
    "    count = (sfd._ddf['pixel_Norder'] == norder).sum().compute()\n",
    "    count_real = pq.read_metadata(PARQUET_DIR / f'pixel_Norder={norder:02d}.parquet').num_rows\n",
    "    print(norder, count - count_real) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e2a6ec6883896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:36:58.588490Z",
     "start_time": "2023-12-01T19:36:14.151868Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "index = sfd._ddf['_hipscat_index'].to_dask_array(lengths=True)\n",
    "display(da.sum(da.diff(index) <= 0).compute())\n",
    "index.argmin().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674787f1881191e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T19:41:00.897996Z",
     "start_time": "2023-12-01T19:40:07.235242Z"
    }
   },
   "outputs": [],
   "source": [
    "from hipscat.pixel_math.hipscat_id import healpix_to_hipscat_id\n",
    "\n",
    "index = sfd._ddf['_hipscat_index'].to_dask_array(lengths=True)\n",
    "diff_index = da.diff(index)\n",
    "diff_index_from_norder = sfd._ddf['pixel_Norder'].to_dask_array(lengths=True).astype(np.uint64).map_blocks(lambda order: healpix_to_hipscat_id(order, 1))[:-1]\n",
    "\n",
    "da.sum((diff_index != diff_index_from_norder).astype(np.uint64)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fe2a2b9503878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfd-venv-python3.9",
   "language": "python",
   "name": "sfd-venv-python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
