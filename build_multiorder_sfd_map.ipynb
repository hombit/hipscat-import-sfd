{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import healpy\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from astropy.coordinates import SkyCoord\n",
    "from dustmaps.sfd import SFDQuery\n",
    "from joblib import Parallel, delayed\n",
    "from hipscat.pixel_math.hipscat_id import healpix_to_hipscat_id, HIPSCAT_ID_COLUMN\n",
    "\n",
    "from mom_builder import gen_mom_from_fn\n",
    "from paths import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e825fbc4a724a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create a function to query SFD dust map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199651b566073946",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sfd_query = SFDQuery(INPUT_DIR)\n",
    "\n",
    "\n",
    "def ebv(norder: int, index_range=None):\n",
    "    n_size = healpy.order2nside(norder)\n",
    "    n_pix = healpy.order2npix(norder)\n",
    "\n",
    "    if not isinstance(index_range, np.ndarray):\n",
    "        if index_range is None:\n",
    "            index_range = (0, n_pix)\n",
    "        if index_range[1] > n_pix:\n",
    "            index_range = (index_range[0], n_pix)\n",
    "        index_range = np.arange(*index_range)\n",
    "    index_range = np.asarray(index_range, dtype=int)\n",
    "\n",
    "    ra, dec = healpy.pix2ang(n_size, index_range, nest=True, lonlat=True)\n",
    "    coord = SkyCoord(ra=ra, dec=dec, unit='deg')\n",
    "\n",
    "    return sfd_query(coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ba1ab54a422fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Context manager to write tiles to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e320fda3b46f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Writer:\n",
    "    \"\"\"Write tiles to parquet files\n",
    "    \n",
    "    It doesn't optimize Parquet group size for now.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path=PARQUET_DIR):\n",
    "        self.path = path\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.parquet_writers = {}\n",
    "\n",
    "    def _create_parquet_writer(self, norder):\n",
    "        path = self.path / f'pixel_norder={norder:02d}.parquet'\n",
    "        return pq.ParquetWriter(\n",
    "            path,\n",
    "            pa.schema([\n",
    "                pa.field(HIPSCAT_ID_COLUMN, pa.uint64()),\n",
    "                pa.field('pixel_Norder', pa.uint8()),\n",
    "                pa.field('pixel_Npix', pa.uint64()),\n",
    "                pa.field('ebv', pa.float32()),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        for writer in self.parquet_writers.values():\n",
    "            writer.close()\n",
    "\n",
    "    def write(self, norder, indexes, values):\n",
    "        hipscat_index = healpix_to_hipscat_id(norder, indexes)\n",
    "        table = pa.Table.from_arrays(\n",
    "            [hipscat_index, np.full(hipscat_index.shape, norder, dtype=np.uint8), indexes, values],\n",
    "            names=[HIPSCAT_ID_COLUMN, 'pixel_Norder', 'pixel_Npix', 'ebv']\n",
    "        )\n",
    "\n",
    "        if norder not in self.parquet_writers:\n",
    "            self.parquet_writers[norder] = self._create_parquet_writer(norder)\n",
    "        self.parquet_writers[norder].write_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef64fb3c662ad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create intermediate parquet files for multiorder map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c018beed6a52ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "max_norder = 17\n",
    "threshold = 0.16 / 2 ** (max_norder - 13)\n",
    "subtree_norder = max(max_norder - 12, 1)\n",
    "\n",
    "\n",
    "def worker(n_jobs, parallel):\n",
    "    def fn(norder, rng):\n",
    "        n_batch = len(rng) // n_jobs\n",
    "        batches = parallel([\n",
    "            delayed(ebv)(norder, rng[i:i + n_batch])\n",
    "            for i in range(0, len(rng), n_batch)\n",
    "        ])\n",
    "        return np.concatenate(batches)\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(PARQUET_DIR, ignore_errors=True)\n",
    "\n",
    "with Parallel(n_jobs=12, backend=\"threading\") as parallel:\n",
    "    worker = worker(parallel.n_jobs, parallel)\n",
    "    with Writer() as writer:\n",
    "        for tiles in gen_mom_from_fn(\n",
    "                worker,\n",
    "                max_norder=max_norder,\n",
    "                split_norder=subtree_norder,\n",
    "                merger=threshold,\n",
    "        ):\n",
    "            writer.write(*tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ef76ef5a8b02f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "area_desired = 12 * 4 ** max_norder\n",
    "area_actual = sum(pq.read_metadata(f).num_rows * 4 ** (max_norder - o) for o in range(0, max_norder + 1) if\n",
    "                  (f := PARQUET_DIR / f'pixel_norder={o:02d}.parquet').exists())\n",
    "\n",
    "assert area_actual == area_desired, f'{area_actual} != {area_desired}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0def0-6a14-4257-bb20-506368403152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
